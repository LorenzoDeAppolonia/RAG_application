{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from playground_secret_key import SECRET_KEY\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.schema import MetadataMode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\", max_tokens=512)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import(\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "\n",
    "from llama_index.extractors.entity import EntityExtractor # takes a long time\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "# doesn't work and there is no documentation\n",
    "# class AuthorExtractor(BaseExtractor):\n",
    "#     def extract(self, nodes):\n",
    "#         metadata_list = [\n",
    "#             {'author' : (\n",
    "#                 node.metadata.get('author',default='Unknown')\n",
    "#             +\n",
    "#             '\\n'\n",
    "#             +\n",
    "#             node.metadata['excerpt_keywords'])\n",
    "#             }\n",
    "#         for node in nodes\n",
    "#         ]\n",
    "#         return metadata_list\n",
    "#\n",
    "#     def aextract(self, nodes):\n",
    "#         pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
    ")\n",
    "\n",
    "\n",
    "extractors = [TitleExtractor(nodes=5, llm=llm),\n",
    "QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "# SummaryExtractor(llm=llm), # not needed here, makes longer summary than slide\n",
    "KeywordExtractor(llm=llm)]\n",
    "\n",
    "transformations = [text_splitter] + extractors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader(input_files=['../data/01_introduction_to_SL-4.pdf']).load_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "'Overﬁtting\\nIssue with evaluation on training data\\nIf we allow ourselves to build very complex models , we can always be\\nas accurate as we like on the training set\\nThe only measure of whether an algorithm will perform well on new\\ndata is the evaluation on the test set\\nOverﬁtting\\nWe expect simple models to generalize better to new data. Therefore,\\nwe always want to ﬁnd the simplest model.\\nBuilding a model that is too complex for the amount of information\\nwe have, is called overﬁtting\\nMichela Papandrea (SUPSI) Introduction to Supervised Learning 21 / 25'"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[20].text # slide 21"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "docs_front_page = docs[:2]\n",
    "docs_content = docs[2:]\n",
    "# don't know why\n",
    "docs = docs_front_page + docs_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "100%|██████████| 25/25 [00:09<00:00,  2.59it/s]\n",
      "100%|██████████| 25/25 [00:04<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# metadata pipeline\n",
    "ing_pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "docs_nodes = ing_pipeline.run(documents=docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "{'page_label': '24',\n 'file_name': '01_introduction_to_SL-4.pdf',\n 'file_path': '..\\\\data\\\\01_introduction_to_SL-4.pdf',\n 'file_type': 'application/pdf',\n 'file_size': 790104,\n 'creation_date': '2024-03-07',\n 'last_modified_date': '2024-03-07',\n 'document_title': '\"Balancing Model Complexity and Dataset Size in Supervised Learning: A Comprehensive Analysis\"',\n 'questions_this_excerpt_can_answer': '1. How does the variation of inputs in a training dataset impact the complexity of the model that can be used in supervised learning without overfitting?\\n2. What role does dataset size play in determining the complexity of models that can be built in supervised learning?\\n3. Why is it important to have a diverse range of data points in a dataset when building complex models in supervised learning, as opposed to simply duplicating or collecting very similar data points?',\n 'excerpt_keywords': 'Supervised Learning, Model Complexity, Dataset Size, Overfitting, Training Dataset'}"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_nodes[-2].metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.core.question_gen.prompts import DEFAULT_SUB_QUESTION_PROMPT_TMPL\n",
    "\n",
    "question_gen = LLMQuestionGenerator.from_defaults(llm=llm,\n",
    "                                                  prompt_template_str=\"\"\"\n",
    "        Follow the example, but instead of giving a question, always prefix the question\n",
    "        with: 'By first identifying and quoting the most relevant sources, '.\n",
    "        \"\"\" + DEFAULT_SUB_QUESTION_PROMPT_TMPL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes=docs_nodes)\n",
    "engine = index.as_query_engine(similarity_top_k=10, llm=OpenAI(model=\"gpt-4\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "final_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        QueryEngineTool(query_engine=engine,metadata=ToolMetadata(name='ml_documents',description='introduction to machine learning'))],question_gen=question_gen,use_async=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 sub questions.\n",
      "\u001B[1;3;38;2;237;90;200m[ml_documents] Q: By first identifying and quoting the most relevant sources, What classification examples were used in the 'introduction to machine learning' document\n",
      "\u001B[0m\u001B[1;3;38;2;90;149;237m[ml_documents] Q: By first identifying and quoting the most relevant sources, What regression examples were used in the 'introduction to machine learning' document\n",
      "\u001B[0m\u001B[1;3;38;2;90;149;237m[ml_documents] A: The document provides two examples of regression. The first example is about predicting a person's annual income based on their education, age, and where they live. The second example is related to the basic linear regression equation, where the outcome is predicted based on the features of the data.\n",
      "\u001B[0m\u001B[1;3;38;2;237;90;200m[ml_documents] A: The document provides several examples of classification tasks. One example is identifying the zip code from handwritten digits on an envelope, where the input is a scan of the handwriting and the output is the actual digits in the zip code. Another example is determining whether a tumor is benign based on a medical image, with the input being the image and the output being whether the tumor is benign or not. The document also mentions binary classification, such as email spam identification, and multiclass classification, like predicting the language of a website from its text.\n",
      "\u001B[0m{\n",
      "    \"classification_examples\": [\n",
      "        \"Identifying zip code from handwritten digits on an envelope\",\n",
      "        \"Determining whether a tumor is benign based on a medical image\",\n",
      "        \"Email spam identification\",\n",
      "        \"Predicting the language of a website from its text\"\n",
      "    ],\n",
      "    \"regression_examples\": [\n",
      "        \"Predicting a person's annual income based on education, age, and location\",\n",
      "        \"Basic linear regression equation for outcome prediction based on data features\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = final_engine.query(\n",
    "    \"\"\"\n",
    "    What classification and regression examples were used? Give your answer as a JSON.\n",
    "    \"\"\"\n",
    ")\n",
    "print(response.response)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
